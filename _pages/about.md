---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

About me
======
My name is **Lan Wu**. I am an **incoming Lecturer in Robotics** (equivalent to Assistant Professor) at the **University of Western Australia (UWA)** (starting **February 2026**). 
My research focuses on **probabilistic perception, mapping, and scene representations** to enable reliable and intelligent robot autonomy.

I received my PhD degree in robotics in 2023 from the Robotics Institute (RI) at the **University of Technology Sydney (UTS)**, where I was supervised by [Prof. Teresa Vidal-Calleja](https://profiles.uts.edu.au/Teresa.VidalCalleja) and [A/Prof. Alen Alempijevic](https://profiles.uts.edu.au/alen.alempijevic). Since completing my PhD, I have been working as a Postdoctoral Research Fellow at the UTS Robotics Institute.

- **1 paper** in IEEE Transactions on Robotics (**T-RO**), first author
- **6 papers** in IEEE Robotics and Automation Letters (**RA-L**)
- Multiple publications at **ICRA** and **IROS**
- In 2024, I was recognised as an [RSS Pioneer](https://sites.google.com/view/rsspioneers2024/participants?authuser=0), a distinction awarded to early-career robotics researchers worldwide
- I serve as a reviewer for T-RO, RA-L, JFR, ICRA and IROS
- I serve as **Associate Editor for ICRA 2025 and 2026** in the Localisation and Mapping session

Research interests
======
My research focuses on robotic perception, 3D scene understanding and representation, SLAM and mapping, dynamic environment modelling, and active perception. I also work with alternative and multi-modal sensing, including LiDAR, RGB-D, sonar and radar.

Applications span advanced manufacturing, field robotics, human–robot collaboration, and assistive technologies such as bionic visual–spatial devices for people who are blind or vision-impaired.

**My long-term goal is to develop model-based, learning-based, or combined model- and learning-based representations that unify scene understanding and decision-making, enabling robots to operate reliably and intelligently in complex real-world environments.**

- Robotic perception and 3D scene understanding  
- SLAM and 3D mapping  
- Dynamic environment modelling
- Active perception and view planning  
- Robotic manipulation and motion planning  
- Multi-modal sensor fusion (LiDAR, cameras, sonar, radar)  


Selected key research outcomes (2020-2025)
======
* **IDMP - Interactive Distance Field Mapping and Planning to Enable Human-Robot Collaboration** <br />
IEEE Robotics and Automation Letters 2024 <br />
Usama Ali\*, Lan Wu\*, Adrian Muller, Fouad Sukkar, Tobias Kaupp and Teresa Vidal-Calleja <br />
\* These authors are co-first authors and contributed equally to this work.  <br />
[arXiv](https://arxiv.org/abs/2403.09988) | 
[Paper](https://ieeexplore.ieee.org/document/10720114) | 
[Video](https://uts-ri.github.io/IDMP/) | 
[Code](https://github.com/UTS-RI/IDMP)

<img width="800" src='/images/IDMP.png'>

* **Log-GPIS-MOP: A Unified Representation for Mapping, Odometry and Planning** <br />
IEEE Transactions on Robotics, vol. 39, pp. 4078-4094, October 2023 <br />
Lan Wu, Ki Myung Brian Lee, Cedric Le Gentil and Teresa Vidal-Calleja <br />
Please find our paper here: [paper](https://arxiv.org/pdf/2206.09506.pdf) and [video](https://www.youtube.com/watch?v=rKdH5Hjkdec)
<img width="800" src='/images/LogGPISMop.png'>

* **Faithful Euclidean Distance Field from Log-Gaussian Process Implicit Surfaces** <br />
IEEE Robotics and Automation Letters, vol. 6, no. 2, pp. 2461–2468, 2021 <br />
Lan Wu, Ki Myung Brian Lee, Liyang Liu and Teresa Vidal-Calleja <br />
Please find our [paper](https://arxiv.org/pdf/2010.11487.pdf) and [video](https://www.youtube.com/watch?v=mypDDuTcrTA&t=574s). 
<img width="800" src='/images/LogGPIS.png'>

* **Skeleton-Based Conditionally Independent Gaussian Process Implicit Surfaces for Fusion in Sparse to Dense 3D Reconstruction** <br />
IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 1532–1539, 2020 <br />
Lan Wu, Raphael Falque, Victor Perez-Puchalt, Liyang Liu, Nico Pietroni and Teresa Vidal-Calleja <br />
Please find our [paper](https://ieeexplore.ieee.org/abstract/document/8968326) and [video](https://www.youtube.com/watch?v=4T85RcNuijQ).
<img width="800" src='/images/skeleton.png'>

Robotic projects I have contributed to (2020–2025)
======
* **Surface Reconstruction for the Meat and Livestock Project** <br />
I applied my research outcomes in the meat and livestock project for trait estimation, funded by the Australian Government Department of Agriculture&Water Resources as part of its Rural R&D for Profit programme, Meat and Livestock Australia under the Grant V.RDP.2005. By using the probabilistic mapping approaches developed during my PhD, my main objective was to convert and process noisy and incomplete point clouds and depth images of non-rigid livestock and rigid carcass into 3D meshes for further processing.
<img width="800" src='/images/OTE.png'>

* **Multi-sensor Perception, Calibration and Synchronisation for a Fast Off-road Vehicle** <br />
I also contributed to UTS' next generation of fast-wheeled robots. During my studies, UTS built FORV, a Fast Off-Road Vehicle for Defence Applications and I was responsible for establishing the multi-sensor fusion platform for mapping and navigation. Calibrating and working across several visual sensors including a binocular camera, a 3D LiDAR and an inertial measurement unit, we managed to have a data collection and synchronisation system based on NVIDIA Jetson and ROS.
<img width="800" src='/images/forv.png'>

* **Simultaneous Localisation and Mapping for Locomotive No.1** <br />
I had a precious opportunity to work with my colleagues to measure the digital representation of the historical train [Locomotive No. 1](https://www.maas.museum/event/locomotive-no-1/) in the [Powerhouse Museum](https://www.maas.museum/powerhouse-museum/). The datasets were collected by LiDAR sensor and RGB-D camera along with an IMU sensor. We operated the sensors around the train to have high-quality observations and then utilise various SLAM frameworks to produce an accurate surface of the train. Moreover, I generated the colored point cloud of the full train using RGB information for LiDAR measurements.
<img width="800" src='/images/train.png'>


Contact
======

**iriswu076@gmail.com**

**If you are interested in pursuing a PhD with me, please email your CV.**

